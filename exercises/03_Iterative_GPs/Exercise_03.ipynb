{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481f631e-a05e-46f3-bb5d-b1112213a276",
   "metadata": {},
   "source": [
    "# Numerics of Machine Learning\n",
    "## Exercise Sheet No. 3 — Computation-Aware Gaussian Process Inference\n",
    "\n",
    "---\n",
    "University of Tübingen, Winter Term 2022/23\n",
    "&copy; N. Bosch, J. Grosse, P. Hennig, A. Kristiadi, M. Pförtner, J. Schmidt, F. Schneider, L. Tatzel, J. Wenger, 2022 CC BY-NC-SA 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4e3a1",
   "metadata": {},
   "source": [
    "PLEASE FILL OUT FOR ALL TEAM MEMBERS:\n",
    "- Last name, first name, Matrikelnr.\n",
    "- Last name, first name, Matrikelnr.\n",
    "\n",
    "The homework must be submitted as a `.pdf` on ILIAS. We do not grade `.ipynb` submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb25740-6c35-4867-94d9-2bae48cc598d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T12:04:56.600777Z",
     "iopub.status.busy": "2022-08-04T12:04:56.599916Z",
     "iopub.status.idle": "2022-08-04T12:05:00.470507Z",
     "shell.execute_reply": "2022-08-04T12:05:00.470290Z",
     "shell.execute_reply.started": "2022-08-04T12:04:56.600598Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tueplots import bundles\n",
    "\n",
    "plt.rcParams.update(bundles.beamer_moml())\n",
    "plt.rcParams.update({\"figure.dpi\":200})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8ab6c",
   "metadata": {},
   "source": [
    "## Computation-Aware Gaussian Process Inference\n",
    "\n",
    "This week we will try to get a better understanding of the computation-aware iterative Gaussian process approximation framework described in the lecture. We will again use the implementation of the IterGP framework from last time. If you installed the package last week, you can skip the first step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb0c624",
   "metadata": {},
   "source": [
    "**Task:** Install [IterGP](https://github.com/JonathanWenger/itergp), a package for iterative Gaussian process inference from our lab:\n",
    "\n",
    "```sh\n",
    "pip uninstall probnum\n",
    "git clone https://github.com/JonathanWenger/itergp\n",
    "cd itergp\n",
    "pip install .\n",
    "```\n",
    "\n",
    "Should you have trouble with the installation, please open an [issue on GitHub](https://github.com/JonathanWenger/itergp/issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itergp\n",
    "from probnum import backend, randprocs, linops, problems, randvars, linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776d2d8",
   "metadata": {},
   "source": [
    "We begin by generating a synthetic dataset from a known latent function. For this sheet you can adjust the size of the dataset to your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent function\n",
    "fun = lambda x: backend.sin(x * 2 * backend.pi)\n",
    "\n",
    "# Generate data\n",
    "rng_state = backend.random.rng_state(42)\n",
    "\n",
    "num_data = 10\n",
    "input_shape = ()\n",
    "output_shape = ()\n",
    "\n",
    "rng_state, rng_state_data = backend.random.split(rng_state, num=2)\n",
    "data = itergp.datasets.SyntheticDataset(\n",
    "    rng_state=rng_state,\n",
    "    size=(num_data, num_data // 2),\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    noise_var=0.1,\n",
    "    fun=fun,\n",
    ")\n",
    "X = data.train.X\n",
    "y = data.train.y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9d3d4",
   "metadata": {},
   "source": [
    "Next, we will define the Gaussian process prior to use and the likelihood, describing our observation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itergp import GaussianProcess\n",
    "from probnum.randprocs import kernels, mean_fns\n",
    "\n",
    "# Model\n",
    "mean_fn = mean_fns.Zero(input_shape=input_shape, output_shape=output_shape)\n",
    "kernel = kernels.Matern(input_shape=input_shape, nu=1.5, lengthscale=0.15)\n",
    "gp = GaussianProcess(mean_fn, kernel)\n",
    "\n",
    "# Likelihood\n",
    "sigma_sq = 0.1\n",
    "noise = randvars.Normal(\n",
    "    mean=backend.zeros(y.shape),\n",
    "    cov=linops.Scaling(sigma_sq, shape=(num_data, num_data)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646304e6",
   "metadata": {},
   "source": [
    "We can now choose one of the iterative approximation methods defined in [`itergp.methods`](https://itergp.readthedocs.io/en/latest/api/methods.html) to compute the combined posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04248d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itergp import methods\n",
    "\n",
    "# Approximation method\n",
    "maxiter = 10\n",
    "approx_method = methods.Cholesky(maxrank = maxiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined posterior\n",
    "gp_post = gp.condition_on_data(X=X, Y=y, b=noise, approx_method=approx_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_post(data.test.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28dfa3",
   "metadata": {},
   "source": [
    "**Task**: Plot the true function, the mathematical posterior and the computational and combined uncertainty of the Gaussian process approximation via IterGP after 5 iterations for the partial Cholesky and the conjugate gradient method. You can make use of our implementations of the [iterative approximation methods](https://itergp.readthedocs.io/en/latest/api/methods.html).\n",
    "\n",
    "_Hint: The plotting function [`gp.plot`](https://itergp.readthedocs.io/en/latest/api/automod/itergp.ConditionalGaussianProcess.html#itergp.ConditionalGaussianProcess.plot) of an (approximate) Gaussian process (conditioned on data) may be useful._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72591b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "Xnew = backend.linspace(-1.5, 1.5, 1000)\n",
    "gp_post_true = gp.condition_on_data(X, y, b=noise, approx_method=methods.Cholesky())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f7482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a36930",
   "metadata": {},
   "source": [
    "**Questions:** \n",
    "1. Which two functions lie with high probability within the computational, and the combined uncertainty, respectively?\n",
    "\n",
    "    _Answer:_ \n",
    "\n",
    "2. Does the combined uncertainty always contain the mathematical uncertainty? If yes, why? If no, why not?\n",
    "\n",
    "    _Answer:_ \n",
    "\n",
    "3. Which method converges fastest to the mathematical posterior? Does that come with drawbacks?\n",
    "   \n",
    "   _Answer:_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b40e6b",
   "metadata": {},
   "source": [
    "## Implementing a Policy\n",
    "\n",
    "We saw in the lecture that the choice of action in each iteration, given by the policy, strongly influences the convergence to the mathematical posterior. In this second part, we will implement a variant of the PseudoInput policy, which at each iteration returns an action $s_i = \\text{Policy}()$ given by\n",
    "$$\n",
    "s_i = k(X, x_i)\n",
    "$$\n",
    "where $X \\in \\mathbb{R}^{n \\times d}$ is the entire training dataset and $x_i$ the datapoint selected at iteration $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f44e2",
   "metadata": {},
   "source": [
    "We begin by initializing the quantities needed for the probabilistic linear solver at the heart of IterGP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51608842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itergp.linops import KernelMatrix\n",
    "\n",
    "# Linear system to solve\n",
    "Khat = KernelMatrix(kernel=kernel + kernels.WhiteNoise(input_shape=input_shape, sigma_sq=sigma_sq), x0=X)\n",
    "problem = problems.LinearSystem(Khat, y)\n",
    "\n",
    "# Prior for probabilistic linear solver\n",
    "P = linops.Zero(shape=(X.shape[0], X.shape[0]))\n",
    "Pinv = linops.Zero(shape=(X.shape[0], X.shape[0]))\n",
    "linsys_prior = linalg.solvers.beliefs.LinearSystemBelief(\n",
    "    x=randvars.Normal(mean=Pinv @ problem.b, cov=Khat.inv()),\n",
    "    Ainv=Pinv,\n",
    "    A=P,\n",
    ")\n",
    "\n",
    "# Initial solver state for experimentation\n",
    "solver_state = linalg.solvers.LinearSolverState(problem=problem, prior=linsys_prior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd52555",
   "metadata": {},
   "source": [
    "The solver's internal state is tracked in `solver_state` which contains quantities like the current `step` and the `problem` definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solver_state.step)\n",
    "solver_state.next_step()\n",
    "solver_state.next_step()\n",
    "print(solver_state.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55e155",
   "metadata": {},
   "source": [
    "**Task:** Complete the implementation of the \"PseudoInput\" policy below. When called it should return the kernel function centered at the datapoint $x_i$ as defined above.\n",
    "\n",
    "_Note: If you are curious, feel free to implement your own idea for a policy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b341d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from probnum import backend\n",
    "from probnum.backend.random import RNGState\n",
    "from probnum.linalg.solvers import policies\n",
    "\n",
    "\n",
    "class CustomPseudoInputPolicy(policies.LinearSolverPolicy):\n",
    "    \"\"\"Custom PseudoInputPolicy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X \n",
    "        Training data.\n",
    "    kernel\n",
    "        Prior covariance function.\n",
    "    \"\"\"\n",
    "    def __init__(self, X : backend.Array, kernel : randprocs.kernels.Kernel) -> None:\n",
    "        self.X = X\n",
    "        self.kernel = kernel\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        solver_state: \"probnum.linalg.solvers.LinearSolverState\",\n",
    "        rng: Optional[RNGState] = None,\n",
    "    ) -> backend.Array:\n",
    "        \"\"\"Return an action for a given solver state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        solver_state\n",
    "            Current state of the linear solver.\n",
    "        rng\n",
    "            Random number generator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action\n",
    "            Next action to take.\n",
    "        \"\"\"\n",
    "        return None # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434a2e4",
   "metadata": {},
   "source": [
    "**Task:** Test out your policy by calling it and seeing what actions it returns. If your implementation is correct, all points should lie on the plot of the kernel function centered at $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e54886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy\n",
    "my_policy = CustomPseudoInputPolicy(X=X, kernel=kernel)\n",
    "\n",
    "# Generate action from policy\n",
    "action = my_policy(solver_state)\n",
    "\n",
    "# Plot\n",
    "plt.axvline(X[solver_state.step], color=\"black\")\n",
    "plt.scatter(X, action)\n",
    "plt.plot(\n",
    "    backend.linspace(-1, 1, 100),\n",
    "    kernel(backend.linspace(-1, 1, 100), X[solver_state.step]),\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a881997",
   "metadata": {},
   "source": [
    "**Task:** Compile your custom probabilistic linear solver by passing the custom policy to the super class constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Custom probabilistic linear solver.\"\"\"\n",
    "import probnum\n",
    "from probnum.linalg.solvers import ProbabilisticLinearSolver, information_ops\n",
    "from itergp.methods import belief_updates, policies, stopping_criteria\n",
    "\n",
    "\n",
    "class CustomSolver(ProbabilisticLinearSolver):\n",
    "    r\"\"\"Custom probabilistic linear solver.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    atol\n",
    "        Absolute tolerance.\n",
    "    rtol\n",
    "        Relative tolerance.\n",
    "    maxiter\n",
    "        Maximum number of iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        atol: float = 1e-6,\n",
    "        rtol: float = 1e-6,\n",
    "        maxiter: int = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy=None, # TODO\n",
    "            information_op=information_ops.ProjectedResidualInformationOp(),\n",
    "            belief_update=belief_updates.ProjectedResidualBeliefUpdate(),\n",
    "            stopping_criterion=stopping_criteria.MaxIterationsStoppingCriterion(\n",
    "                maxiter=maxiter, problem_size_factor=1\n",
    "            )\n",
    "            | probnum.linalg.solvers.stopping_criteria.ResidualNormStoppingCriterion(\n",
    "                atol=atol, rtol=rtol\n",
    "            ),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate GP posterior\n",
    "gp_post = gp.condition_on_data(\n",
    "    X=X, Y=y, b=noise, approx_method=CustomSolver(maxiter=8)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1816e",
   "metadata": {},
   "source": [
    "**Task:** Plot the (approximate) GP posterior including the combined and computational uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55556b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ec6fc",
   "metadata": {},
   "source": [
    "**Question:** How does your custom policy compare to the partial Cholesky? How would you change it to improve it compared to the partial Cholesky?\n",
    "\n",
    "_Answer:_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af848c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('numericsofml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "727690f07341efc52a05470468e7847b9508a442486c4bbf0c04473721444b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
