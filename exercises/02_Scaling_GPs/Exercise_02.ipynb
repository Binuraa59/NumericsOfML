{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481f631e-a05e-46f3-bb5d-b1112213a276",
   "metadata": {},
   "source": [
    "# Numerics of Machine Learning\n",
    "## Exercise Sheet No. 2 — Scaling Gaussian Processes\n",
    "\n",
    "---\n",
    "University of Tübingen, Winter Term 2022/23\n",
    "&copy; N. Bosch, J. Grosse, P. Hennig, A. Kristiadi, M. Pförtner, J. Schmidt, F. Schneider, L. Tatzel, J. Wenger, 2022 CC BY-NC-SA 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4e3a1",
   "metadata": {},
   "source": [
    "PLEASE FILL OUT FOR ALL TEAM MEMBERS:\n",
    "- Last name, first name, Matrikelnr.\n",
    "- Last name, first name, Matrikelnr.\n",
    "\n",
    "The homework must be submitted as a `.pdf` on ILIAS. We do not grade `.ipynb` submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb25740-6c35-4867-94d9-2bae48cc598d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T12:04:56.600777Z",
     "iopub.status.busy": "2022-08-04T12:04:56.599916Z",
     "iopub.status.idle": "2022-08-04T12:05:00.470507Z",
     "shell.execute_reply": "2022-08-04T12:05:00.470290Z",
     "shell.execute_reply.started": "2022-08-04T12:04:56.600598Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tueplots import bundles\n",
    "\n",
    "plt.rcParams.update(bundles.beamer_moml())\n",
    "plt.rcParams.update({\"figure.dpi\":200})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8ab6c",
   "metadata": {},
   "source": [
    "## Solving Linear Systems with Kernel Matrices using Iterative Methods\n",
    "\n",
    "In this first exercise we will more closely investigate the linear solve(s) $b \\mapsto \\hat{K}^{-1}b$ necessary for GP inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb0c624",
   "metadata": {},
   "source": [
    "**Task:** Install [IterGP](https://github.com/JonathanWenger/itergp), a package for iterative Gaussian process inference from our lab:\n",
    "\n",
    "```sh\n",
    "pip uninstall probnum\n",
    "git clone https://github.com/JonathanWenger/itergp\n",
    "cd itergp\n",
    "pip install .\n",
    "```\n",
    "\n",
    "Should you have trouble with the installation, please open an [issue on GitHub](https://github.com/JonathanWenger/itergp/issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee9f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itergp\n",
    "from probnum import backend, randprocs, linops, problems, randvars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776d2d8",
   "metadata": {},
   "source": [
    "We begin by generating a synthetic dataset from a known latent function. For this sheet you can adjust the size of the dataset to your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent function\n",
    "fun = lambda x: backend.sin(x * 2 * backend.pi)\n",
    "\n",
    "# Generate data\n",
    "rng_state = backend.random.rng_state(42)\n",
    "\n",
    "num_data = 2000\n",
    "input_shape = ()\n",
    "output_shape = ()\n",
    "\n",
    "rng_state, rng_state_data = backend.random.split(rng_state, num=2)\n",
    "data = itergp.datasets.SyntheticDataset(\n",
    "    rng_state=rng_state,\n",
    "    size=(num_data, 1000),\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    noise_var=0.1,\n",
    "    fun= fun\n",
    ")\n",
    "X = data.train.X\n",
    "y = data.train.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f37a2",
   "metadata": {},
   "source": [
    "The [probnum](https://probnum.readthedocs.io/en/latest/index.html) library provides a way to define kernels of the form $k_{\\sigma^2}(x_0, x_1) = k(x_0, x_1) + \\sigma^2 \\delta_{x_0 = x_1}$ such that $k_{\\sigma^2}(X, X) = \\hat{K}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kernel\n",
    "kernel = randprocs.kernels.Matern(\n",
    "    nu=1.5, input_shape=()\n",
    ") + randprocs.kernels.WhiteNoise(input_shape=(), sigma_sq=10**-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d885118",
   "metadata": {},
   "source": [
    "**Task:** Plot the kernel matrix for the first $30$ training data points. Does the kernel matrix look as you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate kernel matrix\n",
    "Khat = kernel.matrix(X[0:30])\n",
    "\n",
    "# Plot kernel matrix\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a85ecd",
   "metadata": {},
   "source": [
    "**Task:** Plot the condition number of the kernel matrix $K = k(X, X)$ for the kernels Matern($1/2$), Matern($3/2$) and an exponentiated quadratic / RBF kernel for increasing dataset size $n=1$ to $n=1000$. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5120ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b4ea7cc",
   "metadata": {},
   "source": [
    "**Task:** What happens if you consider the damped kernel matrix $\\hat{K} = K + \\sigma^2 I$ for an observation noise of $\\sigma^2=10^{-6}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983d8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbbf7fc4",
   "metadata": {},
   "source": [
    "Next, our goal will be to approximately solve the linear system $\\hat{K} v = y$, which is needed to approximate the posterior mean of a Gaussian process iteratively. We'll compare the partial Cholesky, the method of conjugate gradients and its preconditioned version with respect to their speed of convergence.\n",
    "\n",
    "We begin by again generating a synthetic dataset and defining our kernel matrix. Here, we construct a linear operator, which does not fully form the kernel matrix in memory, but only implements the matrix-vector product $w \\mapsto \\hat{K} w$. This allows us to only use $\\mathcal{O(nd)}$ memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a75489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "num_data = 2000\n",
    "data = itergp.datasets.SyntheticDataset(\n",
    "    rng_state=rng_state,\n",
    "    size=(num_data, 1000),\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    noise_var=0.1,\n",
    "    fun= fun\n",
    ")\n",
    "X = data.train.X\n",
    "y = data.train.y\n",
    "\n",
    "# Model\n",
    "sigma_sq = 0.1\n",
    "kernel = randprocs.kernels.Matern(nu=1.5, input_shape=()) \n",
    "\n",
    "# Lazily evaluated kernel matrix\n",
    "K = itergp.linops.KernelMatrix(kernel=kernel, x0=X)\n",
    "Khat = K + linops.Scaling(factors=sigma_sq, shape=K.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de35f8",
   "metadata": {},
   "source": [
    "**Task:** Compute a matrix-vector product $w \\mapsto \\hat{K}w$ with the (damped) kernel matrix for a dataset of size $n=10^6$. Linear operators support the same Python syntax `A @ b` for matrix-vector multiplication as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95a62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54632dda",
   "metadata": {},
   "source": [
    "We will now solve a linear system with the kernel matrix with the partial Cholesky, CG and preconditioned CG. For quick debugging reduce the size of the dataset depending on your hardware.\n",
    "\n",
    "We begin by setting up the linear system and the solver configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probnum.linalg import solvers\n",
    "\n",
    "# Linear system\n",
    "problem = problems.LinearSystem(Khat, y)\n",
    "\n",
    "# Solver setup\n",
    "P = linops.Zero(shape=(X.shape[0], X.shape[0]))\n",
    "Pinv = linops.Zero(shape=(X.shape[0], X.shape[0]))\n",
    "linsys_prior = solvers.beliefs.LinearSystemBelief(\n",
    "    x=randvars.Normal(mean=Pinv @ problem.b, cov=Khat.inv()),\n",
    "    Ainv=Pinv,\n",
    "    A=P,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177c1dd",
   "metadata": {},
   "source": [
    "Next, we construct a partial Cholesky preconditioner $\\hat{P}_k = L_k L_k^\\top + \\sigma^2 I \\approx \\hat{K}$.\n",
    "\n",
    "**Question:** What is the rank $k$ of the low-rank component of $\\hat{P}_k$ in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itergp.methods import ConjugateGradient, Cholesky, preconditioners\n",
    "\n",
    "# Partial Cholesky preconditioner\n",
    "qoi_belief, _ = Cholesky(maxrank=20).solve(\n",
    "    linsys_prior, problems.LinearSystem(K, y)\n",
    ")\n",
    "\n",
    "precond = preconditioners.DiagonalPlusLowRank(\n",
    "    diagonal=sigma_sq,\n",
    "    low_rank_factor=K @ qoi_belief.Ainv._summands[1].U,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59ce05",
   "metadata": {},
   "source": [
    "Now, we can define our linear solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38669e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear solvers\n",
    "maxiter = 100\n",
    "\n",
    "cholesky = Cholesky(maxrank=maxiter)\n",
    "\n",
    "cg = ConjugateGradient(\n",
    "    precond_inv=None,\n",
    "    maxiter=maxiter,\n",
    "    atol=1e-12,\n",
    "    rtol=1e-12,\n",
    "    reorthogonalization_fn_residual=None,\n",
    ")\n",
    "\n",
    "precond_cg = ConjugateGradient(\n",
    "    precond_inv=precond.inv(),\n",
    "    maxiter=maxiter,\n",
    "    atol=1e-12,\n",
    "    rtol=1e-12,\n",
    "    reorthogonalization_fn_residual=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b0831",
   "metadata": {},
   "source": [
    "**Task**: Solve the linear system $\\hat{K} v = y$ with Cholesky, CG and preconditioned CG and plot the norm of the residual $r_i = y - \\hat{K}v_i$ given by $\\lVert r_i \\rVert_2 = \\lVert \\hat{K} (v - v_i) \\rVert_2 = \\lVert v - v_i \\rVert_{\\hat{K}^\\top \\hat{K}}$.\n",
    "\n",
    "Note, that in the lecture we plotted the error in $\\hat{K}$-norm, i.e. $\\lVert v - v_i \\rVert_{\\hat{K}}$, to understand the convergence of CG. However, this requires us to know the true solution $v$. So instead as a proxy we can track the residual norm, for which we do not need access to $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca018f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear solves\n",
    "_, solver_state_cholesky = cholesky.solve(problem=problem, prior=linsys_prior)\n",
    "_, solver_state_cg = cg.solve(problem=problem, prior=linsys_prior)\n",
    "_, solver_state_precond_cg = precond_cg.solve(problem=problem, prior=linsys_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5af0e",
   "metadata": {},
   "source": [
    "Hint: The linear solvers automatically track the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals for all iterations\n",
    "residuals_cholesky = solver_state_cholesky.residuals[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual error\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360e302",
   "metadata": {},
   "source": [
    "**Question:** What happens if you vary the size of the preconditioner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984303cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e04b799d",
   "metadata": {},
   "source": [
    "## GP Hyperparameter Optimization via Iterative Methods\n",
    "\n",
    "We will now perform full Gaussian process inference including hyperparameter optimization using GPyTorch, a GP library with a PyTorch backend. It heavily relies on the method of conjugate gradients and implements the stochastic estimation of the trace based on matrix-vector multiplication that was described in the lecture.\n",
    "\n",
    "**Task:** Begin by installing GPyTorch (e.g. `pip install gpytorch`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bad9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af0d38",
   "metadata": {},
   "source": [
    "We will again generate a synthetic dataset for experimentation purposes. Feel free to change the dataset size to whatever allows quick experimentation on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19835247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "rng_state = backend.random.rng_state(42)\n",
    "\n",
    "num_data = 2000\n",
    "input_shape = ()\n",
    "output_shape = ()\n",
    "\n",
    "rng_state, rng_state_data = backend.random.split(rng_state, num=2)\n",
    "fun = lambda x: np.arctan(2 * np.pi * x) + x * np.sin(8 * np.pi * x)\n",
    "data = itergp.datasets.SyntheticDataset(\n",
    "    rng_state=rng_state,\n",
    "    size=(num_data, 1000),\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    noise_var=0.1,\n",
    "    fun= fun\n",
    ")\n",
    "\n",
    "X = torch.as_tensor(data.train.X)\n",
    "y = torch.as_tensor(data.train.y)\n",
    "X_test = torch.as_tensor(data.test.X)\n",
    "y_test = torch.as_tensor(data.test.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6021b7a",
   "metadata": {},
   "source": [
    "Next, we set up the Gaussian process model. \n",
    "\n",
    "**Task:** Define the GP model with a scaled Matern($5/2$) kernel and initialize the kernel hyperparameters like so:\n",
    "- outputscale: 3.0\n",
    "- lengthscale: 1.0\n",
    "- noise $\\sigma^2$: 0.01\n",
    "\n",
    "What modeling assumptions do these hyperparameters encode, i.e. what is their interpretation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=None)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "    \"covar_module.outputscale\": torch.tensor(None),\n",
    "    \"covar_module.base_kernel.lengthscale\": torch.tensor(None),\n",
    "    \"likelihood.noise_covar.noise\": torch.tensor(None),\n",
    "}\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X, y, likelihood)\n",
    "model.initialize(**hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e95e02",
   "metadata": {},
   "source": [
    "Now let's optimize the hyperparameters with respect to the log-marginal likelihood. We choose Adam as our optimizer here, but other choices (like L-BFGS or SGD) also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "training_iter = 20\n",
    "\n",
    "# Choice of optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# (Negative) loss function\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Predict on training data\n",
    "    output = model(X)\n",
    "\n",
    "    # Evaluate loss\n",
    "    loss = -mll(output, y)\n",
    "\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # Update hyperparameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c308454",
   "metadata": {},
   "source": [
    "**Task:** Plot the negative log-marginal likelihood (i.e. the loss) on the training data over time. To do so, modify the training loop above. What are the final values for the outputscale, lengthscale and the noise after 10 optimization steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of negative log-marginal likelihood of the training data\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d33e25",
   "metadata": {},
   "source": [
    "Now let's compute the log-marginal likelihood on the test data and plot the final result.\n",
    "\n",
    "**Question:** Does the model generalize well? What could we do to improve generalization performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634cce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with (\n",
    "    torch.no_grad()\n",
    "    and gpytorch.settings.fast_computations(\n",
    "        covar_root_decomposition=True, log_prob=True, solves=True\n",
    "    )\n",
    "    and gpytorch.settings.fast_pred_var(True)\n",
    "):\n",
    "    xs = torch.as_tensor(data.test.X)\n",
    "    f_preds = model(xs)\n",
    "    y_preds = likelihood(f_preds)\n",
    "    print(f\"Log-marginal likelihood of test data: {-mll(f_preds, y_test).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce40a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of data and model\n",
    "\n",
    "with torch.no_grad():\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    idcs = torch.argsort(X_test)\n",
    "    lower, upper = y_preds[idcs].confidence_region()\n",
    "    ax.scatter(X.numpy(), y.numpy(), color=\"gray\", s=2, label=\"train data\")\n",
    "    ax.scatter(X_test.numpy(), y_test.numpy(), color=\"C1\", s=2, label=\"test data\")\n",
    "    ax.fill_between(\n",
    "        X_test[idcs].numpy(), lower.numpy(), upper.numpy(), alpha=0.5, color=\"C0\", label=\"GP uncertainty\"\n",
    "    )\n",
    "    ax.plot(X_test[idcs].numpy(), y_preds[idcs].mean.numpy(), label=\"GP mean\")\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9de00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('numericsofml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "727690f07341efc52a05470468e7847b9508a442486c4bbf0c04473721444b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
