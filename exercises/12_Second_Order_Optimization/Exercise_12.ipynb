{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9fcd16b",
   "metadata": {},
   "source": [
    "# Numerics of Machine Learning\n",
    "# Exercise Sheet No. 12 — Second-Order Optimization\n",
    "\n",
    "---\n",
    "University of Tübingen, Winter Term 2022/23\n",
    "&copy; N. Bosch, J. Grosse, P. Hennig, A. Kristiadi, M. Pförtner, J. Schmidt, F. Schneider, L. Tatzel, J. Wenger, 2022 CC BY-NC-SA 3.0\n",
    "\n",
    "---\n",
    "\n",
    "PLEASE FILL OUT FOR ALL TEAM MEMBERS:\n",
    "- Last name, first name, Matrikelnr.\n",
    "- Last name, first name, Matrikelnr.\n",
    "\n",
    "The homework must be submitted as a `.pdf` on ILIAS. We do not grade `.ipynb` submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fb734",
   "metadata": {},
   "source": [
    "**Overview:** \n",
    "This week's coding exercise is concerned with the Hessian-free method and the role of damping in stochastic second-order optimization. There are three parts:\n",
    "1. First, we will implement a basic version of the Hessian-free optimizer in PyTorch. If you want to know more about this optimizer, you can find the paper [here](https://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf). \n",
    "2. Next, we will apply this optimizer to a small neural network. \n",
    "3. Finally, we will investigate the relation between damping and its dependence on the mini-batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eedb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Make deterministic\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"  # matplotlib bugfix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd065b5",
   "metadata": {},
   "source": [
    "## 1. Hessian-free optimizer in PyTorch\n",
    "\n",
    "Below, we will implement a second-order method that was discussed in the lecture: the Hessian-free (HF) optimizer. This optimizer uses the method of conjugate gradients to approximate the Newton step. Note that a lot of functionality (e.g. the `cg`-method, the GGN-vector product, and functions for converting the model's parameters to a vector and vice versa) is already implemented in `hf_utils.py`. \n",
    "\n",
    "**Your task:** Complete the missing bits of code below (marked with `TODO`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.convert_parameters import parameters_to_vector\n",
    "\n",
    "from hf_utils import (\n",
    "    cg,\n",
    "    ggn_vector_product,\n",
    "    vector_to_parameter_list,\n",
    "    vector_to_trainparams,\n",
    ")\n",
    "\n",
    "\n",
    "class HessianFree(torch.optim.Optimizer):\n",
    "    \"\"\"This class implements a basic version of a Hessian-free optimizer in \n",
    "    Pytorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        damping=0.0,\n",
    "        cg_maxiter=20,\n",
    "        lr=1.0,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"The constructor creates an instance of the `HessianFree` optimizer.\n",
    "        \n",
    "        Args:\n",
    "            params: An iterable of `torch.Tensor`s that represents the \n",
    "                network's parameters.\n",
    "            damping: A scalar multiple of the identity matrix is added to the\n",
    "                curvature matrix when the cg-method is applied. This damping \n",
    "                parameter is kept constant.\n",
    "            cg_max_iter: The maximum number of cg-iterations. \n",
    "            lr: The learning rate. The update step is scaled by this scalar. \n",
    "            verbose (bool): Print information during the computations.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check hyperparameters\n",
    "        if damping < 0.0:\n",
    "            raise ValueError(f\"Invalid damping = {damping}\")\n",
    "        if cg_maxiter < 1:\n",
    "            raise ValueError(f\"Invalid cg_max_iter: {cg_max_iter}\")\n",
    "        if lr < 0.0 or lr > 1.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        \n",
    "        # Call parent class constructor\n",
    "        defaults = dict(damping=damping, cg_maxiter=cg_maxiter, lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # Only one parameter group is supported\n",
    "        if len(self.param_groups) != 1:\n",
    "            error_msg = \"`HessianFree` does not support per-parameter options.\"\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self._group = self.param_groups[0]\n",
    "        self._params = self._group[\"params\"]\n",
    "\n",
    "        # All computations are performed in the subspace of trainable parameters\n",
    "        self._params_list = [p for p in self._params if p.requires_grad]\n",
    "        self.device = self._params_list[0].device\n",
    "        \n",
    "        # Print info \n",
    "        if self.verbose:\n",
    "            num_params = sum(p.numel() for p in self._params_list)\n",
    "            print(\"Number of trainable parameters: \", num_params)\n",
    "            print(\"Device = \", self.device)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _Gv(loss, outputs, params_list, vec):\n",
    "        \"\"\"Return the GGN-vector product with the vector `vec`.\"\"\"\n",
    "        \n",
    "        ### TODO: Implement the GGN-vector product. \n",
    "    \n",
    "    \n",
    "    def step(self, forward):\n",
    "        \"\"\"Perform an update step.\n",
    "        \n",
    "        Args:\n",
    "            forward: This function returns a `(loss, outputs)`-tuple,\n",
    "                where `loss` is the target function value. Here is a pseudo-code\n",
    "                example of the training loop:\n",
    "                ```\n",
    "                for step_idx in range(num_steps):\n",
    "                    inputs, targets = get_minibatch_data()\n",
    "                    \n",
    "                    def forward():\n",
    "                        outputs = model(inputs)\n",
    "                        loss = loss_function(outputs, targets)\n",
    "                        return loss, outputs\n",
    "                    \n",
    "                    opt.step(forward=forward)\n",
    "                ```\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nComputing HF update step...\")\n",
    "        \n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Set up the linear system\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # Evaluate the loss\n",
    "        loss, outputs = forward()\n",
    "        init_loss = loss.item()\n",
    "\n",
    "        # Evaluate the gradient\n",
    "        grad = torch.autograd.grad(\n",
    "            loss, self._params_list, create_graph=True, retain_graph=True\n",
    "        )\n",
    "        grad = parameters_to_vector(grad).detach()\n",
    "        \n",
    "        # Matrix-vector products with the curvature matrix\n",
    "        def mvp(x):\n",
    "            return self._Gv(loss, outputs, self._params_list, x)\n",
    "        \n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Apply cg\n",
    "        # ----------------------------------------------------------------------\n",
    "        damping = self._group[\"damping\"]\n",
    "        cg_maxiter = self._group[\"cg_maxiter\"]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"  Running cg...\")\n",
    "        \n",
    "        ### TODO: Apply cg (with at most `cg_maxiter` iterations) to the damped (!)\n",
    "        ### linear system.\n",
    "\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "        # Parameter update\n",
    "        # ----------------------------------------------------------------------\n",
    "        lr = self._group[\"lr\"]\n",
    "        \n",
    "        ### TODO: Scale the Newton step with the learning rate, update the parameters\n",
    "        \n",
    "        # Print initial and final loss\n",
    "        final_loss = forward()[0].item()\n",
    "        if self.verbose:\n",
    "            msg = f\"  Initial loss = {init_loss:.6f} --> \"\n",
    "            msg += f\"final loss = {final_loss:.6f}\"\n",
    "            print(msg)\n",
    "            \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab55bcc",
   "metadata": {},
   "source": [
    "## 2. Apply HF to a simple test problem\n",
    "\n",
    "Before we can apply our optimizer, we have to define a test problem. We will use a simple neural network on MNIST data.\n",
    "\n",
    "### 2.1 Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data set to this folder\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "# Normalize data\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        (0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Training and test data\n",
    "train_data = MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "test_data = MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "# Data loaders for training and test data\n",
    "def get_train_loader(batch_size):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "def get_test_loader(batch_size):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        test_data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model\n",
    "def get_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28 * 28, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20189a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss-function\n",
    "loss_func = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "loss_func_no_reduction = torch.nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mb_metrics(model, inputs, targets):\n",
    "    \"\"\"Compute the loss and accuracy on the mini-batch defined by `inputs` \n",
    "    and `targets`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Forward pass, compute loss and accuracy\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, targets)\n",
    "        predictions = torch.argmax(outputs, 1)\n",
    "        acc = (predictions == targets).sum() / len(targets)\n",
    "        return loss.item(), acc.item()\n",
    "    \n",
    "\n",
    "def eval_test_metrics(model, batchsize_test):\n",
    "    \"\"\"Compute the loss and accuracy on the test set.\"\"\"\n",
    "    \n",
    "    # Put model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct_counter = 0\n",
    "    data_counter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in get_test_loader(batchsize_test):\n",
    "            \n",
    "            # Forward pass, aggregate loss and correct predictions\n",
    "            outputs = model(inputs)\n",
    "            loss += loss_func_no_reduction(outputs, targets).sum().item()\n",
    "            predictions = torch.argmax(outputs, 1) \n",
    "            correct_counter += (predictions == targets).sum().item()\n",
    "            data_counter += len(targets)\n",
    "    \n",
    "    loss /= data_counter\n",
    "    acc = correct_counter / data_counter\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c943c",
   "metadata": {},
   "source": [
    "### 2.2. Run HF\n",
    "\n",
    "Let's apply the optimizer to our test problem. We have to perform multiple training runs with the HF-optimizer in the next part of the tutorial, so it makes sense to implement a reusable `run_hf`-function. This function runs the Hessian-free optimizer for a given number of steps `num_steps`. It contains a training loop and stores and returns the training metrics (mini-batch loss/accuracy and test loss/accuracy).\n",
    "\n",
    "**Your task:** Complete the missing bits of code below (marked with `TODO`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80407d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mb_metrics(model, inputs, targets, step, results_dict):\n",
    "    \"\"\"Compute the mini-batch loss and accuracy in step `step` and \n",
    "    put the results into `results_dict`.\n",
    "    \"\"\"\n",
    "    loss, acc = eval_mb_metrics(model, inputs, targets)\n",
    "    \n",
    "    # Store results\n",
    "    results_dict[\"step\"].append(step)\n",
    "    results_dict[\"loss\"].append(loss)\n",
    "    results_dict[\"acc\"].append(acc)\n",
    "\n",
    "    \n",
    "def log_test_metrics(model, batchsize_test, step, results_dict):\n",
    "    \"\"\"Compute the test loss and accuracy in step `step` and \n",
    "    put the results into `results_dict`.\n",
    "    \"\"\"\n",
    "    loss, acc = eval_test_metrics(model, batchsize_test)\n",
    "    \n",
    "    # Store results\n",
    "    results_dict[\"step\"].append(step)\n",
    "    results_dict[\"loss\"].append(loss)\n",
    "    results_dict[\"acc\"].append(acc)\n",
    "\n",
    "\n",
    "def run_hf(\n",
    "    model,\n",
    "    opt,\n",
    "    batchsize_train,\n",
    "    batchsize_test,\n",
    "    num_steps,\n",
    "    log_test_metrics_every\n",
    "):\n",
    "    \"\"\"Run the Hessian-free optimizer `opt` for `num_steps` steps. The test\n",
    "    loss and accuracy are evaluated every `log_test_metrics_every` steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Track these quantities\n",
    "    mb_metrics_before = {\"step\": [], \"loss\": [], \"acc\": []}\n",
    "    mb_metrics_after = {\"step\": [], \"loss\": [], \"acc\": []}\n",
    "    test_metrics_after = {\"step\": [], \"loss\": [], \"acc\": []}\n",
    "        \n",
    "    # Training loop\n",
    "    train_loader = get_train_loader(batchsize_train)\n",
    "    step = 0\n",
    "    while True:\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            \n",
    "            # Log before\n",
    "            log_mb_metrics(model, inputs, targets, step, mb_metrics_before)\n",
    "                \n",
    "            # Put model into training mode\n",
    "            model.train()\n",
    "            \n",
    "            ### TODO: Compute and apply the HF step\n",
    "            \n",
    "            # Log after\n",
    "            log_mb_metrics(model, inputs, targets, step, mb_metrics_after)\n",
    "            if step % log_test_metrics_every == 0:\n",
    "                log_test_metrics(model, batchsize_test, step, test_metrics_after)\n",
    "            \n",
    "            # Increment step counter\n",
    "            step += 1\n",
    "            \n",
    "            # Break from training loop after `num_steps` steps\n",
    "            if step >= num_steps:\n",
    "                break\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "    \n",
    "    # Return training metrics\n",
    "    return mb_metrics_before, mb_metrics_after, test_metrics_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee6470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model, optimizer, data\n",
    "model = get_model()\n",
    "opt = HessianFree(\n",
    "    model.parameters(),\n",
    "    damping=1.0,\n",
    "    cg_maxiter=20,\n",
    "    lr=1.0,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Apply the optimizer to the test problem\n",
    "metrics = run_hf(\n",
    "    model, \n",
    "    opt, \n",
    "    batchsize_train=100, \n",
    "    batchsize_test=1000,\n",
    "    num_steps=20, \n",
    "    log_test_metrics_every=1\n",
    ")\n",
    "mb_metrics_before, mb_metrics_after, test_metrics_after = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3b0a1",
   "metadata": {},
   "source": [
    "**Your task:** Visualize the three metrics returned by `run_hf` in two subplots (one containing the losses, the other one the accuracies). Is the per-step progress on the mini-batch always reflected in similar progress on the test set? If not, do you have an explanation for this observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfca785",
   "metadata": {},
   "source": [
    "**Your answer:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf781c8",
   "metadata": {},
   "source": [
    "## 3. Batch size and damping\n",
    "\n",
    "The Hessian-free optimizer uses a stochastic estimate of the gradient and the curvature. The noise on these estimates depends on the size of the mini batch: The more data is used, the more accurate the estimates. If our estimates are very noisy, we might want to restrict the optimizer to smaller updates, i.e. use larger damping. So, the optimal damping seems to depend on the mini-batch size. Let's investigate this dependency in more detail. \n",
    "\n",
    "First, we define a set of five different batch sizes (`50`, `100`, `200`, `400` and `1000`) and a set of four damping values (`5e-2`, `1e-1`, `5e-1`, and `1.0`). For each combination of batch size and damping, we run the HF-optimizer and evaluate the *best* test loss and test accuracy that was achieved during the HF-run. For a fair comparison between multiple runs with different batch sizes, we choose the number of steps such that `batch_size * num_steps` is constant for all runs (see `batch_size_to_steps` below).\n",
    "\n",
    "**Your task:** Complete the missing bits of code below (marked with `TODO`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9096fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh model\n",
    "model_orig = get_model()\n",
    "\n",
    "# Key = batch size, \n",
    "# Value = number of steps\n",
    "batch_size_to_steps = {\n",
    "    50: 40,\n",
    "    100: 20,\n",
    "    200: 10,\n",
    "    400: 5,\n",
    "    1000: 2,\n",
    "}\n",
    "dampings = [5e-2, 1e-1, 5e-1, 1.0]\n",
    "\n",
    "# Best test loss/acc for all batch size damping combinations\n",
    "best_test_loss = torch.empty(len(batch_size_to_steps), len(dampings))\n",
    "best_test_acc = torch.empty(len(batch_size_to_steps), len(dampings))\n",
    "\n",
    "# Go through all combinations of batch size and damping\n",
    "for bs_idx, (batch_size, num_steps) in enumerate(batch_size_to_steps.items()):    \n",
    "    for damping_idx, damping in enumerate(dampings):\n",
    "        \n",
    "        # Set up problem (copy model!!)\n",
    "        model = copy.deepcopy(model_orig)\n",
    "        opt = HessianFree(\n",
    "            model.parameters(),\n",
    "            damping=damping,\n",
    "            cg_maxiter=20,\n",
    "            lr=1.0,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Print info\n",
    "        log_every=int(math.ceil(num_steps/10))\n",
    "        info_msg = f\"batch_size = {batch_size}, num_steps = {num_steps}, \"\n",
    "        info_msg += f\"damping = {damping}, log_every = {log_every}\"\n",
    "        print(info_msg)\n",
    "        \n",
    "        ### TODO: Run HF, store the best test loss/acc achieved within the given \n",
    "        ### number of steps in `best_test_loss`/`best_test_acc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17f22b",
   "metadata": {},
   "source": [
    "**Your task:** Visualize the optimal damping (in terms of test loss and test accuracy) for each batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb186463",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c05b6",
   "metadata": {},
   "source": [
    "**Your task:** For each batch size, plot the best test loss and test accuracy (in two separate plots) achieved using the ideal damping. Write down your observations and possible explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9c29a",
   "metadata": {},
   "source": [
    "**Your answer:** TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
